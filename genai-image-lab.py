# -*- coding: utf-8 -*-
"""Deep Learning -Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b1Zc68I_TTP1z8qymd036tyGazXDGlnP

# PART A
"""



# --- Import Required Libraries ---
import os
import zipfile
import random
from glob import glob
import requests
from tqdm import tqdm
from PIL import Image
import matplotlib.pyplot as plt

# --- Download COCO subset ---
def download_coco_subset(url, filename="train2017.zip"):
    if not os.path.exists(filename):
        print("Downloading COCO...")
        response = requests.get(url, stream=True)
        total_size = int(response.headers.get('content-length', 0))
        with open(filename, "wb") as f:
            for data in tqdm(response.iter_content(1024 * 1024), total=total_size // (1024 * 1024), unit="MB"):
                f.write(data)
    else:
        print("✅ COCO already exists!")

# --- Extract Images ---
def extract_images(zip_file="train2017.zip", output_dir="sample_images", max_images=100):
    os.makedirs(output_dir, exist_ok=True)
    with zipfile.ZipFile(zip_file, "r") as archive:
        jpg_files = [f for f in archive.namelist() if f.endswith(".jpg")]
        for i, file in enumerate(jpg_files[:max_images]):
            archive.extract(file, output_dir)
            original = os.path.join(output_dir, file)
            new_name = os.path.join(output_dir, os.path.basename(file))
            os.rename(original, new_name)
    print(f"✅ Extracted {max_images} images to {output_dir}!")

# --- Show Random Images (Shuffled) ---
def show_images(folder="sample_images", num_images=5):
    image_paths = sorted(glob(os.path.join(folder, "*.jpg")))
    random.shuffle(image_paths)  # Shuffle the image list
    selected_paths = image_paths[:num_images]

    fig, axes = plt.subplots(1, len(selected_paths), figsize=(15, 5))
    for idx, path in enumerate(selected_paths):
        img = Image.open(path)
        axes[idx].imshow(img)
        axes[idx].axis('off')
    plt.tight_layout()
    plt.show()

# --- Full Execution ---
url = "http://images.cocodataset.org/zips/train2017.zip"
download_coco_subset(url)
extract_images("train2017.zip", output_dir="sample_images", max_images=100)  # You can change number here
show_images("sample_images", num_images=5)

# ✅  GAN setup in Google Colab using 64x64 COCO images
import os, zipfile, requests
from tqdm import tqdm
from glob import glob
from PIL import Image
import numpy as np
from numpy import expand_dims, zeros, ones, vstack
from numpy.random import randn, randint
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, Conv2D, UpSampling2D, LeakyReLU, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.preprocessing.image import array_to_img
from tensorflow.keras.callbacks import Callback

# ✅ Step 1: Download COCO Images

def download_coco_subset(url, filename="train2017.zip"):
    if not os.path.exists(filename):
        response = requests.get(url, stream=True)
        total_size = int(response.headers.get('content-length', 0))
        with open(filename, "wb") as f:
            for data in tqdm(response.iter_content(1024 * 1024), total=total_size // (1024 * 1024), unit='MB'):
                f.write(data)

# ✅ Step 2: Extract and preprocess COCO images to 64x64

def extract_and_preprocess_images(zip_filename, output_dir, image_size=(64, 64), max_images=20000):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        with zipfile.ZipFile(zip_filename, "r") as zip_file:
            image_count = 0
            for file_info in zip_file.infolist():
                if file_info.filename.endswith(".jpg"):
                    zip_file.extract(file_info, output_dir)
                    original = os.path.join(output_dir, file_info.filename)
                    new_name = os.path.join(output_dir, file_info.filename.split("/")[-1])
                    os.rename(original, new_name)
                    image_count += 1
                    if image_count >= max_images:
                        break

    image_paths = sorted(glob(os.path.join(output_dir, "*.jpg")))[:max_images]
    images = []
    for path in image_paths:
        try:
            img = Image.open(path).convert("RGB").resize(image_size)
            img_array = (np.asarray(img).astype("float32") - 127.5) / 127.5  # Normalize to [-1, 1]
            images.append(img_array)
        except:
            continue
    return np.array(images)

# ✅ Step 3: Improved Generator with UpSampling2D + Conv2D

def build_generator():
    model = Sequential([
        Input(shape=(128,)),
        Dense(8 * 8 * 256),
        LeakyReLU(0.2),
        BatchNormalization(),
        Reshape((8, 8, 256)),
        UpSampling2D(),
        Conv2D(128, kernel_size=5, padding="same"),
        LeakyReLU(0.2),
        BatchNormalization(),
        UpSampling2D(),
        Conv2D(64, kernel_size=5, padding="same"),
        LeakyReLU(0.2),
        BatchNormalization(),
        UpSampling2D(),
        Conv2D(3, kernel_size=5, padding="same", activation="tanh")
    ])
    return model

# ✅ Step 4: Discriminator

def build_discriminator():
    model = Sequential([
        Input(shape=(64, 64, 3)),
        Conv2D(64, kernel_size=5, strides=2, padding="same"),
        LeakyReLU(0.2),
        Dropout(0.4),
        Conv2D(128, kernel_size=5, strides=2, padding="same"),
        LeakyReLU(0.2),
        Dropout(0.4),
        Conv2D(256, kernel_size=5, strides=2, padding="same"),
        LeakyReLU(0.2),
        Dropout(0.4),
        Flatten(),
        Dense(1, activation="sigmoid")
    ])
    return model

# ✅ Step 5: GAN model class with label smoothing
class GAN(Model):
    def __init__(self, generator, discriminator):
        super().__init__()
        self.generator = generator
        self.discriminator = discriminator

    def compile(self, g_opt, d_opt, g_loss, d_loss):
        super().compile()
        self.g_opt = g_opt
        self.d_opt = d_opt
        self.g_loss = g_loss
        self.d_loss = d_loss

    def train_step(self, real_images):
        batch_size = tf.shape(real_images)[0]
        latent_vectors = tf.random.normal((batch_size, 128))

        with tf.GradientTape() as d_tape:
            fake_images = self.generator(latent_vectors, training=True)
            yhat_real = self.discriminator(real_images, training=True)
            yhat_fake = self.discriminator(fake_images, training=True)
            yhat_combined = tf.concat([yhat_real, yhat_fake], axis=0)
            labels_combined = tf.concat([
                tf.ones_like(yhat_real) * 0.9,
                tf.zeros_like(yhat_fake)
            ], axis=0)
            d_loss = self.d_loss(labels_combined, yhat_combined)

        d_grads = d_tape.gradient(d_loss, self.discriminator.trainable_variables)
        self.d_opt.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))

        with tf.GradientTape() as g_tape:
            fake_images = self.generator(latent_vectors, training=True)
            predictions = self.discriminator(fake_images, training=False)
            g_loss = self.g_loss(tf.ones_like(predictions), predictions)

        g_grads = g_tape.gradient(g_loss, self.generator.trainable_variables)
        self.g_opt.apply_gradients(zip(g_grads, self.generator.trainable_variables))

        return {"d_loss": d_loss, "g_loss": g_loss}

# ✅ Step 6: Save and show generated images
class SaveGeneratedImages(Callback):
    def __init__(self, latent_dim=128, num_images=5):
        self.latent_dim = latent_dim
        self.num_images = num_images
        os.makedirs("generated_samples", exist_ok=True)

    def on_epoch_end(self, epoch, logs=None):
        latent_points = tf.random.normal((self.num_images, self.latent_dim))
        generated_images = self.model.generator(latent_points)
        generated_images = (generated_images + 1) / 2.0
        fig, axes = plt.subplots(1, self.num_images, figsize=(self.num_images * 2, 2))
        for i in range(self.num_images):
            img = array_to_img(generated_images[i])
            axes[i].imshow(img)
            axes[i].axis('off')
        plt.tight_layout()
        plt.show()

# ✅ Step 7: Execute GAN training pipeline
url = "http://images.cocodataset.org/zips/train2017.zip"
download_coco_subset(url)
images = extract_and_preprocess_images("train2017.zip", "train2017", image_size=(64, 64), max_images=20000)

ds = tf.data.Dataset.from_tensor_slices(images).shuffle(20000).batch(64).prefetch(tf.data.AUTOTUNE)

generator = build_generator()
discriminator = build_discriminator()

gan = GAN(generator, discriminator)
gan.compile(
    g_opt=Adam(learning_rate=0.0002, beta_1=0.5),
    d_opt=Adam(learning_rate=0.0002, beta_1=0.5),
    g_loss=BinaryCrossentropy(),
    d_loss=BinaryCrossentropy()
)

gan.fit(ds, epochs=100, callbacks=[SaveGeneratedImages()])
generator.save("generator_model.h5")

# ✅ Step 8: Generate and save final images after training
from tensorflow.keras.preprocessing.image import array_to_img

# Create output directory if not exists
os.makedirs("generated_samples", exist_ok=True)

# Generate 5 images using the trained generator
num_images = 5
latent_dim = 128
latent_points = tf.random.normal((num_images, latent_dim))
generated_images = gan.generator(latent_points, training=False)
generated_images = (generated_images + 1) / 2.0  # Rescale from [-1,1] to [0,1]

# Save each image
for i in range(num_images):
    img = array_to_img(generated_images[i])
    img_path = f"generated_samples/final_epoch_{i}.png"
    img.save(img_path)
    print(f"✅ Saved: {img_path}")

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/ultralytics/yolov5
# %cd yolov5
!pip install -r requirements.txt

"""# VAE + PixelCNN (Autoregressive Decoder Model)"""



# --- [1] Import Libraries ---
import os, zipfile, requests
from tqdm import tqdm
from glob import glob
from PIL import Image
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
import torchvision.utils as vutils

# --- [2] Download COCO Images ---
def download_coco_subset(url, filename="train2017.zip"):
    if not os.path.exists(filename):
        response = requests.get(url, stream=True)
        total_size = int(response.headers.get('content-length', 0))
        with open(filename, "wb") as f:
            for data in tqdm(response.iter_content(1024 * 1024), total=total_size // (1024 * 1024), unit='MB'):
                f.write(data)

# --- [3] Extract & Preprocess COCO Images ---
def extract_and_preprocess_images(zip_filename, output_dir, image_size=(64, 64), max_images=20000):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        with zipfile.ZipFile(zip_filename, "r") as zip_file:
            image_count = 0
            for file_info in zip_file.infolist():
                if file_info.filename.endswith(".jpg"):
                    zip_file.extract(file_info, output_dir)
                    original = os.path.join(output_dir, file_info.filename)
                    new_name = os.path.join(output_dir, file_info.filename.split("/")[-1])
                    os.rename(original, new_name)
                    image_count += 1
                    if image_count >= max_images:
                        break
    image_paths = sorted(glob(os.path.join(output_dir, "*.jpg")))[:max_images]
    images = []
    for path in image_paths:
        try:
            img = Image.open(path).convert("RGB").resize(image_size)
            img_array = np.asarray(img).astype("float32") / 255.0
            images.append(img_array)
        except:
            continue
    return np.array(images)

# --- [4] Define VAE ---
class VAE(nn.Module):
    def __init__(self, latent_dim=256):
        super(VAE, self).__init__()
        self.latent_dim = latent_dim
        self.enc_conv1 = nn.Conv2d(3, 32, 4, 2, 1)
        self.enc_conv2 = nn.Conv2d(32, 64, 4, 2, 1)
        self.enc_conv3 = nn.Conv2d(64, 128, 4, 2, 1)
        self.fc1 = nn.Linear(8*8*128, latent_dim)
        self.fc2 = nn.Linear(8*8*128, latent_dim)
        self.fc3 = nn.Linear(latent_dim, 8*8*128)
        self.dec_conv1 = nn.ConvTranspose2d(128, 64, 4, 2, 1)
        self.dec_conv2 = nn.ConvTranspose2d(64, 32, 4, 2, 1)
        self.dec_conv3 = nn.ConvTranspose2d(32, 3, 4, 2, 1)

    def encode(self, x):
        x = F.relu(self.enc_conv1(x))
        x = F.relu(self.enc_conv2(x))
        x = F.relu(self.enc_conv3(x))
        x = torch.flatten(x, start_dim=1)
        return self.fc1(x), self.fc2(x)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        x = F.relu(self.fc3(z))
        x = x.view(-1, 128, 8, 8)
        x = F.relu(self.dec_conv1(x))
        x = F.relu(self.dec_conv2(x))
        x = torch.sigmoid(self.dec_conv3(x))
        return x

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

# --- [5] Define PixelCNN ---
class PixelCNN(nn.Module):
    def __init__(self, input_channels=3):
        super(PixelCNN, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 64, 7, padding=3)
        self.blocks = nn.ModuleList([nn.Conv2d(64, 64, 3, padding=1) for _ in range(10)])
        self.final = nn.Conv2d(64, 3, 1)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        for block in self.blocks:
            x = F.relu(block(x))
        return torch.sigmoid(self.final(x))

# --- [6] Define VAE Loss ---
def vae_loss(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

# --- [7] Download COCO + Prepare Dataset ---
url = "http://images.cocodataset.org/zips/train2017.zip"
download_coco_subset(url)
images = extract_and_preprocess_images("train2017.zip", "train2017", image_size=(64, 64), max_images=20000)

x_train = torch.tensor(images).permute(0, 3, 1, 2)
train_loader = DataLoader(TensorDataset(x_train), batch_size=64, shuffle=True)

# --- [8] Initialize Models ---
vae = VAE()
pixelcnn = PixelCNN()

vae_optimizer = optim.Adam(vae.parameters(), lr=0.0002)
pixelcnn_optimizer = optim.Adam(pixelcnn.parameters(), lr=0.0002)

# --- [9] Train VAE ---
vae.train()
for epoch in range(30):   # Lighter VAE training
    total_loss = 0
    for batch_idx, (data,) in enumerate(train_loader):
        vae_optimizer.zero_grad()
        recon_batch, mu, logvar = vae(data)
        loss = vae_loss(recon_batch, data, mu, logvar)
        loss.backward()
        vae_optimizer.step()
        total_loss += loss.item()
    print(f"✅ VAE Epoch {epoch+1}, Loss: {total_loss/len(train_loader.dataset):.4f}")

# --- [10] Generate 5000 VAE Outputs ---
vae.eval()
with torch.no_grad():
    z = torch.randn(5000, 256)
    vae_outputs = vae.decode(z)

# --- [11] Train PixelCNN on 5000 VAE Outputs ---
pixelcnn.train()
pixelcnn_loader = DataLoader(TensorDataset(vae_outputs, vae_outputs), batch_size=64, shuffle=True)

for epoch in range(10):  # PixelCNN stronger training
    total_loss = 0
    for batch_idx, (vae_out, real_img) in enumerate(pixelcnn_loader):
        pixelcnn_optimizer.zero_grad()
        refined = pixelcnn(vae_out.detach())
        loss = F.binary_cross_entropy(refined, real_img)
        loss.backward()
        pixelcnn_optimizer.step()
        total_loss += loss.item()
    print(f"✅ PixelCNN Epoch {epoch+1}, Loss: {total_loss/len(pixelcnn_loader.dataset):.4f}")

# --- [12] Generate and Display 5 Final Refined Images ---
pixelcnn.eval()
with torch.no_grad():
    sample = vae_outputs[:5]  # Pick first 5 coarse images
    refined = pixelcnn(sample)

    grid = vutils.make_grid(refined, nrow=5, padding=5, normalize=True)
    plt.figure(figsize=(12, 4))
    plt.imshow(grid.permute(1,2,0).cpu().numpy())
    plt.axis('off')
    plt.title("🖼️ Final Refined Images After VAE + PixelCNN")
    plt.show()

import os
print("Current directory:", os.getcwd())
print("Subfolders:", os.listdir())

# Commented out IPython magic to ensure Python compatibility.
# Install Selective Search
!pip install -q git+https://github.com/AlpacaDB/selectivesearch.git
!git clone https://github.com/ultralytics/yolov5
# %cd yolov5
!pip install -r requirements.txt
# R-CNN Pipeline
import cv2, numpy as np, matplotlib.pyplot as plt, selectivesearch, os
from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions
from keras.models import Model
from PIL import Image

# Load VGG16 model
model = VGG16(weights='imagenet')
image_paths = [f"/content/final_epoch_{i}.png" for i in range(5)]
os.makedirs("rcnn_outputs", exist_ok=True)

for img_index, path in enumerate(image_paths):
    print(f"\n🔍 Processing: {path}")
    image = cv2.imread(path)
    if image is None:
        print("Could not load image."); continue

    # Resize for speed
    h, w = image.shape[:2]; scale = 600 / max(h, w)
    image = cv2.resize(image, (int(w * scale), int(h * scale)))
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Apply Selective Search
    _, regions = selectivesearch.selective_search(image_rgb, scale=500, sigma=0.9, min_size=10)
    candidates = set()
    for r in regions:
        x, y, w, h = r['rect']
        if w * h < 3000 or w == 0 or h == 0 or w/h > 3 or h/w > 3: continue
        candidates.add((x, y, w, h))
        if len(candidates) >= 40: break

    results = []
    for (x, y, w, h) in candidates:
        roi = image[y:y+h, x:x+w]
        if roi.shape[0] < 20 or roi.shape[1] < 20: continue
        resized = cv2.resize(roi, (224, 224))
        input_tensor = preprocess_input(np.expand_dims(resized.astype(np.float32), axis=0))
        preds = model.predict(input_tensor, verbose=0)
        label, conf = decode_predictions(preds, top=1)[0][0][1], decode_predictions(preds, top=1)[0][0][2]
        if conf > 0.3: results.append((conf, label, x, y, w, h))

    print(f" Found {len(results)} objects")
    if results:
        for conf, label, x, y, w, h in sorted(results, reverse=True)[:5]:
            print(f" - {label} ({conf:.2f}) at [{x},{y},{w},{h}]")
            cv2.rectangle(image_rgb, (x, y), (x+w, y+h), (255, 0, 0), 2)
            cv2.putText(image_rgb, f"{label} ({conf:.2f})", (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
    else:
        cv2.putText(image_rgb, "No confident objects found", (20, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)

    plt.figure(figsize=(10, 7))
    plt.imshow(image_rgb); plt.axis('off')
    plt.title(f"R-CNN Detection: final_epoch_{img_index}.png")
    plt.show()

    out_path = f"rcnn_outputs/rcnn_detected_{img_index}.png"
    cv2.imwrite(out_path, cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR))
    print(f"💾 Saved: {out_path}")

from PIL import Image

os.makedirs("resized_samples", exist_ok=True)

for i in range(5):
    img = Image.open(f"generated_samples/final_epoch_{i}.png")
    img = img.resize((640, 640))
    img.save(f"resized_samples/resized_{i}.png")
    print(f"✅ Resized: resized_samples/resized_{i}.png")

# Install YOLOv5 dependencies and load model
import torch
import matplotlib.pyplot as plt

# Load YOLOv5s model from Ultralytics via torch.hub
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

# Loop through GAN-resized images and detect
for i in range(5):
    path = f"resized_samples/resized_{i}.png"
    print(f"\n🔍 Detecting objects in: {path}")


    results = model(path, conf=0.05)

    # Print and show results
    results.print()
    results.show()          # Display bounding boxes

import torch

# Load YOLOv5 model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

# Detect objects
for i in range(5):
    path = f"resized_samples/resized_{i}.png"
    results = model(path)
    results.print()     # Print detected objects
    results.show()      # Show image with boxes

from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

# Load BLIP
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Caption images
for i in range(5):
    img = Image.open(f"resized_samples/resized_{i}.png").convert("RGB")
    inputs = processor(img, return_tensors="pt")
    out = blip_model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)
    print(f" resized_{i}.png ➤ Caption: {caption}")

from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

for i in range(5):
    path = f"resized_samples/resized_{i}.png"
    image = Image.open(path).convert("RGB")
    inputs = processor(image, return_tensors="pt")
    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)
    print(f"🖼 resized_{i}.png ➤ Caption: {caption}")

!pip install -q fiftyone torch torchvision transformers pillow matplotlib

import fiftyone as fo, fiftyone.zoo as foz
import torch, matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from transformers import BlipProcessor, BlipForConditionalGeneration

# Load 5 random OpenImagesV7 images
dataset = foz.load_zoo_dataset("open-images-v7", split="validation", label_types=["detections"], max_samples=5, shuffle=True)
image_paths = [sample.filepath for sample in dataset]

# Load models
device = "cuda" if torch.cuda.is_available() else "cpu"
detector = fasterrcnn_resnet50_fpn(pretrained=True).eval().to(device)
caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
caption_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

# COCO labels
COCO_LABELS = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',
    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',
    'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',
    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet',
    'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',
    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',
    'hair drier', 'toothbrush'
]

# Functions
def detect_objects_with_boxes(image):
    img_tensor = transforms.ToTensor()(image).unsqueeze(0).to(device)
    with torch.no_grad():
        pred = detector(img_tensor)[0]
    boxes = pred['boxes'].cpu().numpy()
    labels = [COCO_LABELS[i] for i, s in zip(pred['labels'], pred['scores']) if s > 0.6]
    scores = pred['scores'].cpu().numpy()
    filtered_boxes = [box for i, box in enumerate(boxes) if scores[i] > 0.6]
    return labels, filtered_boxes

def generate_caption(image):
    inputs = caption_processor(images=image, return_tensors="pt").to(device)
    out = caption_model.generate(**inputs)
    return caption_processor.decode(out[0], skip_special_tokens=True)

# Run detection + captioning with bounding boxes
for path in image_paths:
    img = Image.open(path).convert("RGB")
    caption = generate_caption(img)
    labels, boxes = detect_objects_with_boxes(img)

    fig, ax = plt.subplots(1, figsize=(8, 6))
    ax.imshow(img)
    ax.set_title(caption)

    for box, label in zip(boxes, labels):
        x1, y1, x2, y2 = box
        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='red', linewidth=2)
        ax.add_patch(rect)
        ax.text(x1, y1, label, fontsize=10, color='white', bbox=dict(facecolor='red', alpha=0.5))

    plt.axis("off")
    plt.tight_layout()
    plt.show()



